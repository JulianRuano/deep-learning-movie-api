{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5hKSK5N_6O_",
        "outputId": "eb23bb28-7fd6-44e7-e874-2fd3a7a440a5"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow-recommenders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p31W1Jlw9xPB"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_recommenders as tfrs\n",
        "from collections import Counter\n",
        "from typing import Dict, Text\n",
        "from ast import literal_eval\n",
        "from datetime import datetime\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38bZftla91rp"
      },
      "outputs": [],
      "source": [
        "credits = pd.read_csv('/content/drive/MyDrive/dataset/credits.csv')\n",
        "keywords = pd.read_csv('/content/drive/MyDrive/dataset/keywords.csv')\n",
        "movies = pd.read_csv('/content/drive/MyDrive/dataset/movies_metadata.csv').\\\n",
        "                     drop(['belongs_to_collection', 'homepage', 'imdb_id', 'poster_path', 'status', 'title', 'video'], axis=1).\\\n",
        "                     drop([19730, 29503, 35587]) # Incorrect data type\n",
        "\n",
        "movies['id'] = movies['id'].astype('int64')\n",
        "\n",
        "df = movies.merge(keywords, on='id').\\\n",
        "    merge(credits, on='id')\n",
        "\n",
        "df['original_language'] = df['original_language'].fillna('')\n",
        "df['runtime'] = df['runtime'].fillna(0)\n",
        "df['tagline'] = df['tagline'].fillna('')\n",
        "\n",
        "df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WE_OcB9-C7o"
      },
      "outputs": [],
      "source": [
        "def get_text(text, obj='name'):\n",
        "    text = literal_eval(text)\n",
        "\n",
        "    if len(text) == 1:\n",
        "        for i in text:\n",
        "            return i[obj]\n",
        "    else:\n",
        "        s = []\n",
        "        for i in text:\n",
        "            s.append(i[obj])\n",
        "        return ', '.join(s)\n",
        "\n",
        "df['genres'] = df['genres'].apply(get_text)\n",
        "df['production_companies'] = df['production_companies'].apply(get_text)\n",
        "df['production_countries'] = df['production_countries'].apply(get_text)\n",
        "df['crew'] = df['crew'].apply(get_text)\n",
        "df['spoken_languages'] = df['spoken_languages'].apply(get_text)\n",
        "df['keywords'] = df['keywords'].apply(get_text)\n",
        "\n",
        "# New columns\n",
        "df['characters'] = df['cast'].apply(get_text, obj='character')\n",
        "df['actors'] = df['cast'].apply(get_text)\n",
        "\n",
        "df.drop('cast', axis=1, inplace=True)\n",
        "df = df[~df['original_title'].duplicated()]\n",
        "df = df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4_vTDZu-DnN"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-BCbJBYB6NN"
      },
      "outputs": [],
      "source": [
        "ratings_df = pd.read_csv('/content/drive/MyDrive/dataset/ratings_small.csv')\n",
        "\n",
        "ratings_df['date'] = ratings_df['timestamp'].apply(lambda x: datetime.fromtimestamp(x))\n",
        "ratings_df.drop('timestamp', axis=1, inplace=True)\n",
        "\n",
        "ratings_df = ratings_df.merge(df[['id', 'original_title', 'genres', 'overview']], left_on='movieId',right_on='id', how='left')\n",
        "ratings_df = ratings_df[~ratings_df['id'].isna()]\n",
        "ratings_df.drop('id', axis=1, inplace=True)\n",
        "ratings_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "#ratings_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6YVK6CoB77W"
      },
      "outputs": [],
      "source": [
        "movies_df = df[['id', 'original_title']]\n",
        "movies_df.rename(columns={'id':'movieId'}, inplace=True)\n",
        "# movies_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saWlEEmTB9Sp"
      },
      "outputs": [],
      "source": [
        "ratings_df['userId'] = ratings_df['userId'].astype(str)\n",
        "\n",
        "ratings = tf.data.Dataset.from_tensor_slices(dict(ratings_df[['userId', 'original_title', 'rating']]))\n",
        "movies = tf.data.Dataset.from_tensor_slices(dict(movies_df[['original_title']]))\n",
        "\n",
        "ratings = ratings.map(lambda x: {\n",
        "    \"original_title\": x[\"original_title\"],\n",
        "    \"userId\": x[\"userId\"],\n",
        "    \"rating\": float(x[\"rating\"])\n",
        "})\n",
        "\n",
        "movies = movies.map(lambda x: x[\"original_title\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOB56noBB-2t",
        "outputId": "7621cc7e-4262-4fd0-c6bc-054620918aa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Data: 43188\n"
          ]
        }
      ],
      "source": [
        "print('Total Data: {}'.format(len(ratings)))\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
        "\n",
        "train = ratings.take(35_000)\n",
        "test = ratings.skip(35_000).take(8_188)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NzRzeFZCApF",
        "outputId": "04355671-0ff3-4150-bbf5-0a8fdbde0605"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique Movies: 42373\n",
            "Unique users: 671\n"
          ]
        }
      ],
      "source": [
        "movie_titles = movies.batch(1_000)\n",
        "user_ids = ratings.batch(1_000).map(lambda x: x[\"userId\"])\n",
        "\n",
        "unique_movie_titles = np.unique(np.concatenate(list(movie_titles)))\n",
        "unique_user_ids = np.unique(np.concatenate(list(user_ids)))\n",
        "\n",
        "print('Unique Movies: {}'.format(len(unique_movie_titles)))\n",
        "print('Unique users: {}'.format(len(unique_user_ids)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THCCOiQwCFKu"
      },
      "outputs": [],
      "source": [
        "class MovieModel(tfrs.models.Model):\n",
        "\n",
        "  def __init__(self, rating_weight: float, retrieval_weight: float) -> None:\n",
        "    # We take the loss weights in the constructor: this allows us to instantiate\n",
        "    # several model objects with different loss weights.\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    embedding_dimension = 64\n",
        "\n",
        "    # User and movie models.\n",
        "    self.movie_model: tf.keras.layers.Layer = tf.keras.Sequential([\n",
        "      tf.keras.layers.StringLookup(\n",
        "        vocabulary=unique_movie_titles, mask_token=None),\n",
        "      tf.keras.layers.Embedding(len(unique_movie_titles) + 1, embedding_dimension)\n",
        "    ])\n",
        "    self.user_model: tf.keras.layers.Layer = tf.keras.Sequential([\n",
        "      tf.keras.layers.StringLookup(\n",
        "        vocabulary=unique_user_ids, mask_token=None),\n",
        "      tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
        "    ])\n",
        "\n",
        "    # A small model to take in user and movie embeddings and predict ratings.\n",
        "    # We can make this as complicated as we want as long as we output a scalar\n",
        "    # as our prediction.\n",
        "    self.rating_model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(256, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(1),\n",
        "    ])\n",
        "\n",
        "    # The tasks.\n",
        "    self.rating_task: tf.keras.layers.Layer = tfrs.tasks.Ranking(\n",
        "        loss=tf.keras.losses.MeanSquaredError(),\n",
        "        metrics=[tf.keras.metrics.RootMeanSquaredError()],\n",
        "    )\n",
        "    self.retrieval_task: tf.keras.layers.Layer = tfrs.tasks.Retrieval(\n",
        "        metrics=tfrs.metrics.FactorizedTopK(\n",
        "            candidates=movies.batch(128).map(self.movie_model)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # The loss weights.\n",
        "    self.rating_weight = rating_weight\n",
        "    self.retrieval_weight = retrieval_weight\n",
        "\n",
        "  def call(self, features: Dict[Text, tf.Tensor]) -> tf.Tensor:\n",
        "    # We pick out the user features and pass them into the user model.\n",
        "    user_embeddings = self.user_model(features[\"userId\"])\n",
        "    # And pick out the movie features and pass them into the movie model.\n",
        "    movie_embeddings = self.movie_model(features[\"original_title\"])\n",
        "\n",
        "    return (\n",
        "        user_embeddings,\n",
        "        movie_embeddings,\n",
        "        # We apply the multi-layered rating model to a concatentation of\n",
        "        # user and movie embeddings.\n",
        "        self.rating_model(\n",
        "            tf.concat([user_embeddings, movie_embeddings], axis=1)\n",
        "        ),\n",
        "    )\n",
        "\n",
        "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
        "\n",
        "    ratings = features.pop(\"rating\")\n",
        "\n",
        "    user_embeddings, movie_embeddings, rating_predictions = self(features)\n",
        "\n",
        "    # We compute the loss for each task.\n",
        "    rating_loss = self.rating_task(\n",
        "        labels=ratings,\n",
        "        predictions=rating_predictions,\n",
        "    )\n",
        "    retrieval_loss = self.retrieval_task(user_embeddings, movie_embeddings)\n",
        "\n",
        "    # And combine them using the loss weights.\n",
        "    return (self.rating_weight * rating_loss\n",
        "            + self.retrieval_weight * retrieval_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCczz11ICHwt",
        "outputId": "73385748-458f-4d62-d429-2c5e9bb36df6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "35/35 [==============================] - 145s 4s/step - root_mean_squared_error: 1.5053 - factorized_top_k/top_1_categorical_accuracy: 3.4286e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0083 - factorized_top_k/top_10_categorical_accuracy: 0.0208 - factorized_top_k/top_50_categorical_accuracy: 0.1044 - factorized_top_k/top_100_categorical_accuracy: 0.1727 - loss: 6814.0551 - regularization_loss: 0.0000e+00 - total_loss: 6814.0551\n",
            "Epoch 2/15\n",
            "35/35 [==============================] - 134s 4s/step - root_mean_squared_error: 1.0244 - factorized_top_k/top_1_categorical_accuracy: 9.7143e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0211 - factorized_top_k/top_10_categorical_accuracy: 0.0460 - factorized_top_k/top_50_categorical_accuracy: 0.2028 - factorized_top_k/top_100_categorical_accuracy: 0.3269 - loss: 6452.8895 - regularization_loss: 0.0000e+00 - total_loss: 6452.8895\n",
            "Epoch 3/15\n",
            "35/35 [==============================] - 136s 4s/step - root_mean_squared_error: 0.9896 - factorized_top_k/top_1_categorical_accuracy: 7.4286e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0264 - factorized_top_k/top_10_categorical_accuracy: 0.0590 - factorized_top_k/top_50_categorical_accuracy: 0.2480 - factorized_top_k/top_100_categorical_accuracy: 0.3841 - loss: 6178.2357 - regularization_loss: 0.0000e+00 - total_loss: 6178.2357\n",
            "Epoch 4/15\n",
            "35/35 [==============================] - 134s 4s/step - root_mean_squared_error: 0.9832 - factorized_top_k/top_1_categorical_accuracy: 5.1429e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0299 - factorized_top_k/top_10_categorical_accuracy: 0.0681 - factorized_top_k/top_50_categorical_accuracy: 0.2857 - factorized_top_k/top_100_categorical_accuracy: 0.4356 - loss: 5974.9134 - regularization_loss: 0.0000e+00 - total_loss: 5974.9134\n",
            "Epoch 5/15\n",
            "35/35 [==============================] - 133s 4s/step - root_mean_squared_error: 0.9536 - factorized_top_k/top_1_categorical_accuracy: 4.0000e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0320 - factorized_top_k/top_10_categorical_accuracy: 0.0766 - factorized_top_k/top_50_categorical_accuracy: 0.3206 - factorized_top_k/top_100_categorical_accuracy: 0.4720 - loss: 5810.5497 - regularization_loss: 0.0000e+00 - total_loss: 5810.5497\n",
            "Epoch 6/15\n",
            "35/35 [==============================] - 134s 4s/step - root_mean_squared_error: 0.9360 - factorized_top_k/top_1_categorical_accuracy: 3.7143e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0376 - factorized_top_k/top_10_categorical_accuracy: 0.0895 - factorized_top_k/top_50_categorical_accuracy: 0.3445 - factorized_top_k/top_100_categorical_accuracy: 0.4920 - loss: 5677.0265 - regularization_loss: 0.0000e+00 - total_loss: 5677.0265\n",
            "Epoch 7/15\n",
            "35/35 [==============================] - 133s 4s/step - root_mean_squared_error: 0.9247 - factorized_top_k/top_1_categorical_accuracy: 4.2857e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0437 - factorized_top_k/top_10_categorical_accuracy: 0.0967 - factorized_top_k/top_50_categorical_accuracy: 0.3612 - factorized_top_k/top_100_categorical_accuracy: 0.5118 - loss: 5577.6781 - regularization_loss: 0.0000e+00 - total_loss: 5577.6781\n",
            "Epoch 8/15\n",
            "35/35 [==============================] - 132s 4s/step - root_mean_squared_error: 0.9321 - factorized_top_k/top_1_categorical_accuracy: 0.0014 - factorized_top_k/top_5_categorical_accuracy: 0.0468 - factorized_top_k/top_10_categorical_accuracy: 0.1033 - factorized_top_k/top_50_categorical_accuracy: 0.3749 - factorized_top_k/top_100_categorical_accuracy: 0.5325 - loss: 5499.1362 - regularization_loss: 0.0000e+00 - total_loss: 5499.1362\n",
            "Epoch 9/15\n",
            "35/35 [==============================] - 132s 4s/step - root_mean_squared_error: 0.9006 - factorized_top_k/top_1_categorical_accuracy: 0.0028 - factorized_top_k/top_5_categorical_accuracy: 0.0483 - factorized_top_k/top_10_categorical_accuracy: 0.1054 - factorized_top_k/top_50_categorical_accuracy: 0.3864 - factorized_top_k/top_100_categorical_accuracy: 0.5493 - loss: 5443.9758 - regularization_loss: 0.0000e+00 - total_loss: 5443.9758\n",
            "Epoch 10/15\n",
            "35/35 [==============================] - 132s 4s/step - root_mean_squared_error: 0.8932 - factorized_top_k/top_1_categorical_accuracy: 0.0032 - factorized_top_k/top_5_categorical_accuracy: 0.0486 - factorized_top_k/top_10_categorical_accuracy: 0.1084 - factorized_top_k/top_50_categorical_accuracy: 0.4009 - factorized_top_k/top_100_categorical_accuracy: 0.5688 - loss: 5392.8074 - regularization_loss: 0.0000e+00 - total_loss: 5392.8074\n",
            "Epoch 11/15\n",
            "35/35 [==============================] - 132s 4s/step - root_mean_squared_error: 0.8805 - factorized_top_k/top_1_categorical_accuracy: 0.0030 - factorized_top_k/top_5_categorical_accuracy: 0.0512 - factorized_top_k/top_10_categorical_accuracy: 0.1116 - factorized_top_k/top_50_categorical_accuracy: 0.4109 - factorized_top_k/top_100_categorical_accuracy: 0.5835 - loss: 5352.7351 - regularization_loss: 0.0000e+00 - total_loss: 5352.7351\n",
            "Epoch 12/15\n",
            "35/35 [==============================] - 132s 4s/step - root_mean_squared_error: 0.8727 - factorized_top_k/top_1_categorical_accuracy: 0.0033 - factorized_top_k/top_5_categorical_accuracy: 0.0504 - factorized_top_k/top_10_categorical_accuracy: 0.1128 - factorized_top_k/top_50_categorical_accuracy: 0.4213 - factorized_top_k/top_100_categorical_accuracy: 0.5973 - loss: 5319.5809 - regularization_loss: 0.0000e+00 - total_loss: 5319.5809\n",
            "Epoch 13/15\n",
            "35/35 [==============================] - 130s 4s/step - root_mean_squared_error: 0.8679 - factorized_top_k/top_1_categorical_accuracy: 0.0038 - factorized_top_k/top_5_categorical_accuracy: 0.0506 - factorized_top_k/top_10_categorical_accuracy: 0.1147 - factorized_top_k/top_50_categorical_accuracy: 0.4300 - factorized_top_k/top_100_categorical_accuracy: 0.6093 - loss: 5288.7394 - regularization_loss: 0.0000e+00 - total_loss: 5288.7394\n",
            "Epoch 14/15\n",
            "35/35 [==============================] - 132s 4s/step - root_mean_squared_error: 0.8615 - factorized_top_k/top_1_categorical_accuracy: 0.0033 - factorized_top_k/top_5_categorical_accuracy: 0.0513 - factorized_top_k/top_10_categorical_accuracy: 0.1175 - factorized_top_k/top_50_categorical_accuracy: 0.4365 - factorized_top_k/top_100_categorical_accuracy: 0.6172 - loss: 5265.1153 - regularization_loss: 0.0000e+00 - total_loss: 5265.1153\n",
            "Epoch 15/15\n",
            "35/35 [==============================] - 131s 4s/step - root_mean_squared_error: 0.8537 - factorized_top_k/top_1_categorical_accuracy: 0.0032 - factorized_top_k/top_5_categorical_accuracy: 0.0517 - factorized_top_k/top_10_categorical_accuracy: 0.1172 - factorized_top_k/top_50_categorical_accuracy: 0.4419 - factorized_top_k/top_100_categorical_accuracy: 0.6267 - loss: 5244.5710 - regularization_loss: 0.0000e+00 - total_loss: 5244.5710\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79f77f13eb30>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = MovieModel(rating_weight=1.0, retrieval_weight=1.0)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
        "\n",
        "cached_train = train.shuffle(100_000).batch(1_000)\n",
        "cached_test = test.batch(1_000)\n",
        "\n",
        "model.fit(cached_train, epochs=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QU7tcoAaFMS"
      },
      "outputs": [],
      "source": [
        "model.retrieval_task = tfrs.tasks.Retrieval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2trFrmomQ_VT",
        "outputId": "9943216d-7e81-4402-94ec-abd531d35f3f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow_recommenders.tasks.retrieval.Retrieval object at 0x79f77f13dcc0>, because it is not built.\n"
          ]
        }
      ],
      "source": [
        "model.save(\"movie_recommendation_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5eW1A0Vb1Ev"
      },
      "outputs": [],
      "source": [
        "loaded_model = tf.keras.models.load_model(\"/content/drive/MyDrive/movie_recommendation_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9D1xixZlb112"
      },
      "outputs": [],
      "source": [
        "def predict_movie(user, top_n=3):\n",
        "    # Create a model that takes in raw query features, and\n",
        "    index = tfrs.layers.factorized_top_k.BruteForce(loaded_model.user_model)\n",
        "    # Index from the entire movies dataset.\n",
        "    index.index_from_dataset(\n",
        "        tf.data.Dataset.zip((movies.batch(100), movies.batch(100).map(loaded_model.movie_model)))\n",
        "    )\n",
        "\n",
        "    # Get recommendations.\n",
        "    _, titles = index(tf.constant([str(user)]))\n",
        "\n",
        "    print('Top {} recommendations for user {}:\\n'.format(top_n, user))\n",
        "    for i, title in enumerate(titles[0, :top_n].numpy()):\n",
        "        print('{}. {}'.format(i+1, title.decode(\"utf-8\")))\n",
        "\n",
        "def predict_rating(user, movie):\n",
        "    # Use the trained embeddings directly from the loaded model\n",
        "    trained_movie_embeddings = loaded_model.movie_model(np.array([movie]))\n",
        "    trained_user_embeddings = loaded_model.user_model(np.array([str(user)]))\n",
        "\n",
        "    # Predict the rating using the trained embeddings\n",
        "    predicted_rating = loaded_model.rating_model(\n",
        "        tf.concat([trained_user_embeddings, trained_movie_embeddings], axis=1)\n",
        "    )\n",
        "\n",
        "    print(\"Predicted rating for {}: {}\".format(movie, predicted_rating.numpy()[0][0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqM0896vcDx9",
        "outputId": "b8ca2f42-37fa-4b74-e7d9-5536ad2df586"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 5 recommendations for user 100:\n",
            "\n",
            "1. Shaft in Africa\n",
            "2. Armageddon\n",
            "3. 英雄\n",
            "4. 2001: A Space Odyssey\n",
            "5. Lock, Stock and Two Smoking Barrels\n"
          ]
        }
      ],
      "source": [
        "predict_movie(100, 5)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
